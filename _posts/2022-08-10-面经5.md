---
layout: post
title: 【秋招】数分面经V
subtitle: 逻辑回归损失函数，特征筛选
date: 2022-08-10
author: Zaki
header-img: img/post-bg-universe.jpg
catalog: true
tags:
     - 笔记
     - Interview
---


# 特征筛选


　特征选择方法有很多，一般分为三类：第一类过滤法比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面我们提到的方差筛选就是过滤法的一种。第二类是包装法，根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。
 
最简单的方法就是方差筛选。方差越大的特征，那么我们可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对我们的算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃。在实际应用中，我们会指定一个方差的阈值，当方差小于这个阈值的特征会被我们筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。

# 逻辑回归的损失函数

![](https://s3.bmp.ovh/imgs/2022/08/10/c130afed1d13c40e.png)

5、逻辑回归的优缺点总结：

优点：

形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。

模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。

训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。

资源占用小,尤其是内存。因为只需要存储各个维度的特征值。

方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cut off，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

缺点：

准确率并不是很高。因为形式非常的简单，很难去拟合数据的真实分布。

很难处理数据不平衡的问题。

处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。

逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

6、逻辑回归和线性回归的区别：

逻辑回归是一种广义线性模型，它引入了Sigmod函数，是非线性模型，但本质上还是一个线性回归模型，因为除去Sigmod函数映射关系，其他的算法原理，步骤都是线性回归的。

逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个Sigmod函数，使样本映射到[0,1]之间的数值，从而来处理分类问题。

逻辑回归假设数据服从伯努利分布，线性回归假设数据服从高斯分布；

逻辑回归的损失函数是对数极大似然函数。线性回归是平方损失函数；

逻辑回归输出的是概率，用于分类；线性回归输出的是数值，用于预测；

7、逻辑回归svm区别：

1、损失函数不同，逻辑回归是对数损失函数，svm是hinge-loss；

2、svm主要关注支持向量，也就是和分类最相关的少数点，而逻辑回归关注全局，

线性SVM不直接依赖于数据分布，分类平面不受非支持向量点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing

3、在解决非线性问题的时候，svm一般引入核函数，逻辑回归则不是，

这是因为SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。 而LR则每个点都需要两两计算核函数，计算量太过庞大。

4、逻辑回归产出的是概率值，但是svm只能产出是正类还是负类，不能产出概率；

5、svm基于距离，因此依赖测度，需要对数据进行归一化处理，但是逻辑回归不需要。

6、svm的损失函数自带正则项，因此泛化能力比逻辑回归要好。

8、逻辑回归和决策树什么区别？

1、逻辑回归通常用于分类问题，决策树可回归、可分类。

2、逻辑回归是线性函数，决策树是非线性函数。

3、逻辑回归的表达式很简单，回归系数就确定了模型。决策树的形式就复杂了，叶子节点的范围+取值。两个模型在使用中都有很强的解释性，银行较喜欢。

4、逻辑回归可用于高维稀疏数据场景，比如ctr预估；决策树变量连续最好，类别变量的话，稀疏性不能太高。

5、逻辑回归的核心是sigmoid函数，具有无限可导的优点，常作为神经网络的激活函数。

6、在集成模型中，随机森林、GBDT以决策树为基模型，Boosting算法也可以用逻辑回归作为基模型。

9、逻辑回归为什么要做特征交叉（也成为特征拟合）？

逻辑回归模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。

另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸，过拟合，所以基本特征+特征组合兼顾了全局和个性化。


# SQL时间戳

(1)日期转为时间戳

UNIX_TIMESTAMP('2015-04-29','yyyy-MM-dd')

(2)时间戳转为日期

FROM_UNIXTIME('1430236800','yyyy-MM-dd')

sql查询使用时可将两函数中第一个参数替换为相应的字段名称
