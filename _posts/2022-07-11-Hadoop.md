---
layout: post
title: 【秋招】Hadoop和MapDuce面试笔记
subtitle: Hadoop面试
date: 2022-01-17
author: Zaki
header-img: img/post-bg-universe.jpg
catalog: true
tags:
     - 笔记
     - Interview
---

# 什么是Hadoop 

Hadoop是一个由Apache基金会所开发的分布式系统基础架构, 是一个存储系统+计算框架的软件框架。主要解决海量数据存储与计算的问题，整个Hadoop的核心HDFS和MapReduce。HDFS为海量数据提供了存储，而MapReduce为海量数据提供了计算框架。

# 说说Hadoop生态圈

首先，hadoop生态系统，意思就是以hadoop为平台的各种应用框架，相互兼容，组成了一个独立的应用体系，也可以称之为生态圈。

首先是JDK,java语言的软件开发工具包，是Hadoop最基本的一个服务。HDFS（hadoop分布式文件系统），mapreduce（分布式计算框架），其次是zookeeper（分布式协作服务），解决分布式环境下的数据管理问题：统一命名，状态同步，集群管理，配置同步等。flume（日志收集工具），cloudera提供的开源的日志收集系统，具有分布式，高可靠，高容错，易于定制和扩展的特点。Hive, 基于hadoop的数据仓库。Storm，内存计算框架。可以通过网络直接把数据导入到内存里，直接在内存里计算，流式处理。还有hbase,这是一个面向列的分布式的开源数据库，hbase采用了bigtable的数据模型。Kafka, 这是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。还有redis，c语言编写的，一个内存数据库。sqoop（数据同步工具）。Impala，一个新的查询系统，非常快，可以查询到存储在hdoop上hdfs和hbase上pb级的大数据。

# 说说MapReduce的流程和suffle过程

MapReduce，首先分为Map端，和Reduce端。首先我们有一个待处理的文件，包含了文件的路径，名称，内容和大小。我们有一个客户端，想要跑Mapreduce的程序，我们要把文件提交到集群上，会有一个submit()的方法，在submit之前，获取一下待处理文本的信息，根据参数配置形成一个动态任务的分配规划，比如把文件分成一些数据块，然后提交。提交的地方是yarn, 是集群资源调度的一个组件，提交的信息也包含job.split, wc.jar, job.xml。提交到resourcemanager上，它会根据个数和大小计算，计算出maptask的数量，然后启动。

maptask会调用一个默认的textinputformat，它再会去调用inputformat，它再去调用recorderreader, 调用reader方法，将数据一行一行读取，根据rr将数据抽成一行一行key,value的形式。key就是行首字母的偏移量。value就是一行数据，以k,v的方法会到这个mapper方法里面去。

map（）根据传进来的kv进行逻辑运算，再write()出去，标记好分区，发送到环形缓冲区当中，环形缓冲区的默认大小是100M, 达到80%的阈值，会溢写。溢写会产生大量文件。溢写之前会分区和排序，排序的规则是将k进行字典序排序，快排，分区的规则就是将key取哈希值，哈希值除以reduce task的数量，余几就放到哪个分区里面去。分区内的文件也是有序的，溢写产生的这些大量文件，会再进行merge操作，进行归并排序，默认把10个溢写文件合并成大文件，也可以对数据进行combiner的操作，combiner的结果也不能对最终结果产生影响，等所有maptask完成操作后，启动reduce task。

reducetask会发取拉取的线程到map端拉取数据，一次发送十个，数据先加载到内存里，内存不够就写到磁盘里去，等数据都拉过来了，再进行一次归并排序，归并完的文件进行一次分组的操作，将数据以组为单位发送到reduce()中，做完逻辑运算后，最终会调用outputformat，在调用recorderwriter,将数据以kv形式进行输出。

# Hadoop端口

dfs.namenode.http-address:50070
 SecondaryNameNode 辅助名称节点端口号：50090
dfs.datanode.address:50010
fs.defaultFS:8020 或者 9000 文件系统做交互的端口
yarn.resourcemanager.webapp.address:8088
历史服务器 web 访问端口：19888

# 配置文件
core-site,xml
hdfs.site.xml
mapred-site.xml
yarn-site.xml
hadoop-env.sh
yarn-env.sh
mapred-env.sh
slaves
